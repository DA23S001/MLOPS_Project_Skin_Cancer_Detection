{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_arch_name: str\n",
    "    params_num_classes: int\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_learning_rate: float\n",
    "    params_momentum: float\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Skin_Cancer_Classifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from Skin_Cancer_Classifier.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = self.config.data_ingestion.unzip_dir\n",
    "\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        root_dir_train = Path(training.root_dir)\n",
    "        root_dir_model = Path(prepare_base_model.root_dir)\n",
    "        arch_name = self.params.ARCH_NAME\n",
    "        base_model_path = root_dir_model / f\"model_{arch_name}.pth\"\n",
    "        trained_model_path = root_dir_train / f\"model_{arch_name}.pth\"\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=root_dir_train,\n",
    "            trained_model_path=trained_model_path,\n",
    "            base_model_path= base_model_path,\n",
    "            training_data=Path(training_data),\n",
    "            params_arch_name = arch_name,\n",
    "            params_num_classes= params.CLASSES,\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_learning_rate=params.LEARNING_RATE,\n",
    "            params_momentum= params.MOMENTUM,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE)\n",
    "        \n",
    "\n",
    "        return training_config\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Skin_Cancer_Classifier.components.prepare_base_model import Baseline\n",
    "from Skin_Cancer_Classifier.components.data_ingestion import FedIsic2019, BaselineLoss, acc_metric\n",
    "from torch.utils.data import ConcatDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = Baseline(True, self.config.params_arch_name, self.config.params_num_classes)\n",
    "        self.model.load_state_dict(torch.load(self.config.base_model_path))\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "        trainset = FedIsic2019(train = True, data_path = self.config.training_data, centers = [0, 5])\n",
    "        valset = FedIsic2019(train = False, data_path = self.config.training_data, centers = [0, 5])\n",
    "        self.trainloader = DataLoader(trainset, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "        self.valloader= DataLoader(valset, batch_size=self.config.params_batch_size, shuffle=False)\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.params_learning_rate, momentum=self.config.params_momentum)\n",
    "        criterion = \n",
    "        for _ in range(self.config.params_epochs):\n",
    "            for batch in tqdm(self.trainloader):\n",
    "                batch = list(batch)\n",
    "                images, labels = batch[0], batch[1]\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(net(images.to(device)), labels.to(device))    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class Isic2019Raw(torch.utils.data.Dataset):\n",
    "    \"\"\"Pytorch dataset containing all the features, labels and datacenter\n",
    "    information for Isic2019.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    image_paths: list[str]\n",
    "        the list with the path towards all features\n",
    "    targets: list[int]\n",
    "        the list with all classification labels for all features\n",
    "    centers: list[int]\n",
    "        the list for all datacenters for all features\n",
    "    X_dtype: torch.dtype\n",
    "        the dtype of the X features output\n",
    "    y_dtype: torch.dtype\n",
    "        the dtype of the y label output\n",
    "    augmentations:\n",
    "        image transform operations from the albumentations library,\n",
    "        used for data augmentation\n",
    "    data_path: str\n",
    "        If data_path is given it will ignore the config file and look for the\n",
    "        dataset directly in data_path. Defaults to None.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_dtype :\n",
    "    y_dtype :\n",
    "    augmentations :\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_dtype=torch.float32, y_dtype=torch.int64, augmentations=None, data_path=None,):\n",
    "        \"\"\"\n",
    "        Cf class docstring\n",
    "        \"\"\"\n",
    "\n",
    "        if not (os.path.exists(data_path)):\n",
    "            raise ValueError(f\"The string {data_path} is not a valid path.\")\n",
    "        \n",
    "\n",
    "        self.input_path = data_path\n",
    "\n",
    "        self.dic = {\"input_preprocessed\": os.path.join(self.input_path, \"ISIC_2019_Training_Input_preprocessed\"),\n",
    "                    \"train_test_split\": os.path.join(self.input_path, \"train_test_split\"),}\n",
    "        self.X_dtype = X_dtype\n",
    "        self.y_dtype = y_dtype\n",
    "        df2 = pd.read_csv(self.dic[\"train_test_split\"])\n",
    "        images = df2.image.tolist()\n",
    "        self.image_paths = [os.path.join(self.dic[\"input_preprocessed\"], image_name + \".jpg\") for image_name in images ]\n",
    "        self.targets = df2.target\n",
    "        self.augmentations = augmentations\n",
    "        self.centers = df2.center\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = np.array(Image.open(image_path))\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Image augmentations\n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(image, dtype=self.X_dtype),\n",
    "            torch.tensor(target, dtype=self.y_dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "class FedIsic2019(Isic2019Raw):\n",
    "    \"\"\"\n",
    "    Pytorch dataset containing for each center the features and associated labels\n",
    "    for the Isic2019 federated classification.\n",
    "    One can instantiate this dataset with train or test data coming from either of\n",
    "    the 6 centers it was created from or all data pooled.\n",
    "    The train/test split is fixed and given in the train_test_split file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    center : int, optional\n",
    "        Default to 0\n",
    "    train : bool, optional\n",
    "        Default to True\n",
    "    pooled : bool, optional\n",
    "        Default to False\n",
    "    debug : bool, optional\n",
    "        Default to False\n",
    "    X_dtype : torch.dtype, optional\n",
    "        Default to torch.float32\n",
    "    y_dtype : torch.dtype, optional\n",
    "        Default to torch.int64\n",
    "    data_path: str\n",
    "        If data_path is given it will ignore the config file and look for the\n",
    "        dataset directly in data_path. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train: bool = True, X_dtype: torch.dtype = torch.float32, y_dtype: torch.dtype = torch.int64, data_path: str = None, centers: list = None):\n",
    "        \"\"\"Cf class docstring\"\"\"\n",
    "        sz = 200\n",
    "        if train:\n",
    "            augmentations = albumentations.Compose(\n",
    "                [\n",
    "                    albumentations.RandomScale(0.07),\n",
    "                    albumentations.Rotate(50),\n",
    "                    albumentations.RandomBrightnessContrast(0.15, 0.1),\n",
    "                    albumentations.Flip(p=0.5),\n",
    "                    albumentations.Affine(shear=0.1),\n",
    "                    albumentations.RandomCrop(sz, sz),\n",
    "                    albumentations.CoarseDropout(random.randint(1, 8), 16, 16),\n",
    "                    albumentations.Normalize(always_apply=True),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            augmentations = albumentations.Compose(\n",
    "                [\n",
    "                    albumentations.CenterCrop(sz, sz),\n",
    "                    albumentations.Normalize(always_apply=True),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            X_dtype=X_dtype,\n",
    "            y_dtype=y_dtype,\n",
    "            augmentations=augmentations,\n",
    "            data_path=data_path,\n",
    "        )\n",
    "\n",
    "        self.centers_list = centers\n",
    "        self.train_test = \"train\" if train else \"test\"\n",
    "        df = pd.read_csv(self.dic[\"train_test_split\"])\n",
    "\n",
    "        df2 = df[(df['fold'] == self.train_test) & (df['center'].isin(self.centers_list))].reset_index(drop=True)\n",
    "\n",
    "        images = df2.image.tolist()\n",
    "        self.image_paths = [os.path.join(self.dic[\"input_preprocessed\"], image_name + \".jpg\") for image_name in images]\n",
    "        self.targets = df2.target\n",
    "        self.centers = df2.center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = FedIsic2019(train = True, data_path = '/home/fmlpc/Shashank/Course_Work/MLOPS/MLOPS_Project_Skin_Cancer_Detection/artifacts/data_ingestion', centers = [0, 5])\n",
    "testset = FedIsic2019(train = False, data_path = '/home/fmlpc/Shashank/Course_Work/MLOPS/MLOPS_Project_Skin_Cancer_Detection/artifacts/data_ingestion', centers = [0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10281, 2571)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset.targets), len(testset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0] * 8\n",
    "for x in trainset:\n",
    "    weights[int(x[1])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(trainset)\n",
    "class_weights = torch.FloatTensor([N / weights[i] for i in range(8)])\n",
    "lossfunc = BaselineLoss(alpha=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4.3992,   2.8252,   4.5693,  16.9653,  11.0667, 101.7921, 115.5169,\n",
       "         31.1545])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2337, 3639, 2250, 606, 929, 101, 89, 330]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
